## Обзор

Начиная с [главы 3](../ch03_linear), мы углубленно изучали различные модели от простых до сложных. Хотя эти модели существенно различаются по своей структуре и производительности, их методы обучения и применения имеют много общего: необходимо заранее собирать и готовить тренировочные данные, и модель должна быть хорошо обучена и оптимизирована перед использованием. Можно сказать, что производство модели похоже на вынашивание ребёнка в утробе: искусственный интеллект пока что довольно хрупок и не может полноценно взаимодействовать с внешним миром. Ему требуется относительно закрытая среда для роста. Ключ к дальнейшей эволюции заключается в постоянной адаптации к новому окружению и решении новых задач. Аналогично, обучение модели должно перейти на новый этап, где она будет учиться и развиваться в условиях постоянного взаимодействия.

В этой главе мы обсудим **обучение с подкреплением** (Reinforcement Learning, RL). Обучение с подкреплением — это не новая структура модели, а совершенно новый способ её обучения. В его основе лежит вопрос о том, как обучить модель в неопределённой среде (когда тренировочные данные ещё не полностью собраны). Для решения этой задачи обучение с подкреплением использует уникальную стратегию: начинать использовать модель для собственного обучения, даже если она ещё не полностью готова. Этот подход аналогичен тому, как люди учатся в реальной жизни, например, кататься на велосипеде, совершенствуясь через постоянные попытки и практику.

Обучение с подкреплением охватывает множество тем, и его можно рассматривать как отдельную дисциплину. Поскольку оно требует работы с неопределённой средой, обучение с подкреплением включает в себя множество вероятностных анализов и сложных математических выводов. Подробное изложение всех аспектов потребовало бы объёмной монографии, поэтому в этой главе мы не будем углубляться во все детали, а сосредоточимся на пути развития больших языковых моделей. В частности, мы обсудим, как, следуя примеру ChatGPT, использовать технологию **PPO** (Proximal Policy Optimization) для оптимизации модели. Технология оптимизации, используемая в ChatGPT, находится на переднем крае обучения с подкреплением, поэтому в этой главе будут рассмотрены многие ключевые концепции этой области.

## Описание кода

| Код | Описание |
| --- | --- |
| [intuition_model.ipynb](intuition_model.ipynb) | Интуитивная связь между большой языковой моделью и моделью оценки |
| [utils.py](utils.py) | Определение игры и соответствующих инструментов визуализации |
| [value_learning.ipynb](value_learning.ipynb) | Обучение функции ценности |
| [policy_learning.ipynb](policy_learning.ipynb) | Обучение стратегии |
| [a2c.ipynb](a2c.ipynb) | Базовая линия и модель A2C |
| [llm_ppo.ipynb](llm_ppo.ipynb) | Оптимизация большой языковой модели с использованием алгоритма PPO для улучшения оценки модели после настройки |
| [llm_ppo_correct_dropout.ipynb](llm_ppo_correct_dropout.ipynb) | Аналогично [llm_ppo.ipynb](llm_ppo.ipynb), этот скрипт акцентирует внимание на правильном использовании случайного отключения (dropout) в алгоритме PPO |
