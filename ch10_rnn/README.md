## Обзор

В [главе 8](../ch08_mlp) и [главе 9](ch09_cnn) были подробно рассмотрены многослойный перцептрон и свёрточные нейронные сети. Несмотря на значительные различия в их структуре, с точки зрения обработки данных, они оба исходят из одного основного предположения: данные независимы друг от друга, и модель учитывает только отношения между признаками текущих данных и метками. Такие модели обычно называются **обычными нейронными сетями** (Vanilla Neural Network). Например, свёрточные нейронные сети часто используются для задач распознавания изображений, где каждое изображение обрабатывается независимо, и модель не учитывает возможные связи между ними. Помимо распознавания изображений, свёрточные нейронные сети также могут применяться для классификации текста. Например, они могут использоваться для анализа настроений предложений, классифицируя их на выражающие положительные эмоции (например, «Китайская команда победила в овертайме») и отрицательные эмоции (например, «Мои слёзы не перестают течь»). В этом случае модель также обрабатывает каждое предложение независимо и не учитывает зависимость между ними.

Важно отметить, что не все данные удовлетворяют вышеупомянутому предположению независимости. Например, если предложения для анализа настроений взяты из одного текста, то для понимания одного предложения необходимо учитывать информацию из его контекста, так как одно и то же предложение в разных контекстах может выражать разные эмоции. Например, в тексте «Китайская команда победила в овертайме. Мои слёзы не перестают течь» второе предложение выражает положительные эмоции. Данные с взаимозависимыми элементами называются **последовательными данными** (Sequential Data или Sequence Data). Типичными примерами являются цены на финансовых рынках (временные ряды), текст (последовательности символов) и видео (последовательности изображений).

Для последовательных данных обычные нейронные сети обычно показывают не лучшие результаты. Это связано с тем, что их структура ограничивает способность модели изучать зависимость в данных. Хотя можно использовать некоторые хитроумные методы для улучшения этой способности, их эффективность обычно ограничена, и они могут приводить к другим проблемам моделирования. В [char_mlp.ipynb](char_mlp.ipynb) на конкретном примере будет подробно рассмотрен метод обучения последовательным данным с использованием обычных нейронных сетей, а также его преимущества и недостатки.

Чтобы преодолеть ограничения обычных нейронных сетей при обработке последовательных данных, в академической среде были введены **рекуррентные нейронные сети** (Recurrent Neural Network, RNN). Это совершенно новая структура модели, показывающая впечатляющие результаты в различных контекстах. Особенно в области обработки естественного языка (Natural Language Processing, NLP) эти модели часто превышают ожидания. На самом деле, впечатляющие и даже несколько пугающие большие языковые модели построены на основе рекуррентных нейронных сетей. С этой главы мы сосредоточимся на обработке естественного языка и рекуррентных нейронных сетях, обсуждая, как заставить этот новый вид интеллектуальных систем понимать человеческий язык и извлекать из него знания.

## Описание кода

| Код | Описание |
| --- | --- |
| [tokenizer.ipynb](tokenizer.ipynb) | Результаты токенизации для различных языков |
| [char_mlp.ipynb](char_mlp.ipynb) | Использование многослойного перцептрона для авторегрессионного обучения на естественном языке (предсказание следующей буквы на основе контекста) |
| [embedding_example.ipynb](embedding_example.ipynb) | Простой пример, демонстрирующий детали реализации текстовых встраиваний |
| [char_rnn.ipynb](char_rnn.ipynb) | Использование рекуррентной нейронной сети для авторегрессионного обучения на естественном языке. Этот скрипт реализован не слишком эффективно, но его легко понять |
| [char_rnn_batch.ipynb](char_rnn_batch.ipynb) | Использование рекуррентной нейронной сети для авторегрессионного обучения на естественном языке с поддержкой пакетной обработки |
| [bptt_example.ipynb](bptt_example.ipynb) | Визуализация деталей алгоритма BPTT с помощью вычислительного графа |
| [lstm.ipynb](lstm.ipynb) | Использование сети долгой краткосрочной памяти для авторегрессионного обучения на естественном языке |
